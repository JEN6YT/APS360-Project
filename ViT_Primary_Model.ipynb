{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPR18X7RPR6GOil3w+st5fT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JEN6YT/APS360-Project/blob/main/ViT_Primary_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivZXOKR6x0T3",
        "outputId": "5155ec8b-7ad4-4067-f536-cc25dd057317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMPJEkmGi3sm"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import pandas as pd\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import Pytorch Library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# import matplotlib lirary\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu0sPIvgrSbL",
        "outputId": "58253f93-d4bd-4285-8122-fdba7f1cf2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzql0fKtMUwa"
      },
      "outputs": [],
      "source": [
        "X_train = np.load('/content/drive/My Drive/U of T/APS360 Deep Learning/x_train_data.npy', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.load('/content/drive/My Drive/U of T/APS360 Deep Learning/y_train_data.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "CHRN-rnHlw53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = np.load('/content/drive/My Drive/U of T/APS360 Deep Learning/x_val_data.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "U49mV3iLl1cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val = np.load('/content/drive/My Drive/U of T/APS360 Deep Learning/y_val_data.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "WCanyunOl7gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val = y_val[:20]\n",
        "X_val = X_val[:20]\n",
        "X_train = X_train[:100]\n",
        "y_train = y_train[:100]"
      ],
      "metadata": {
        "id": "WwPh3Og2r80n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.float32(X_train)\n",
        "X_val = np.float32(X_val)"
      ],
      "metadata": {
        "id": "rmhVOBIaje7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.transpose(X_train, (0,3,2,1))\n",
        "X_val = np.transpose(X_val, (0,3,2,1))"
      ],
      "metadata": {
        "id": "opSnhtwYjg54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "VYIAAkpct7Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MyDataset(X_train, y_train)\n",
        "val_dataset = MyDataset(X_val, y_val)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, num_workers=1, shuffle=True)\n",
        "#test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, num_workers=4 , shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, num_workers=1 , shuffle=True)\n",
        "\n",
        "use_cuda = True\n",
        "\n",
        "print(\"Training dataset size: \", len(train_dataset))\n",
        "print(\"Validation dataset size: \", len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cul8Zfm7Xvxn",
        "outputId": "eb4ed1f8-3407-4f5f-ed14-067959ffc746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size:  100\n",
            "Validation dataset size:  20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from einops.layers.torch import Rearrange\n",
        "from einops import repeat\n"
      ],
      "metadata": {
        "id": "tj6wV0FTYrnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        ) # this breaks down the image in s1xs2 patches, and then flat them\n",
        "        \n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
        "\n",
        "        \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1) #prepending the cls token\n",
        "        x += self.positions\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3) # queries, keys and values matrix\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "        \n",
        "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
        "        # split keys, queries and values in num_heads\n",
        "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        # sum up over the last axis\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
        "        \n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "            \n",
        "        scaling = self.emb_size ** (1/2)\n",
        "        \n",
        "        att = F.softmax(energy, dim=-1) / scaling\n",
        "        att = self.att_drop(att)\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values) # sum over the third axis\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        \n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size: int, L: int = 4, drop_p: float = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, L * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(L * emb_size, emb_size),\n",
        "        )\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size: int = 768, drop_p: float = 0., forward_expansion: int = 4,\n",
        "                 forward_drop_p: float = 0.,\n",
        "                 **kwargs):\n",
        "                 \n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, **kwargs),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, L=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))\n",
        "        \n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth: int = 12, **kwargs):\n",
        "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
        "        super().__init__(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size), \n",
        "            nn.Linear(emb_size, n_classes))\n",
        "        \n",
        "class ViT(nn.Sequential):\n",
        "    def __init__(self,     \n",
        "                in_channels: int = 3,\n",
        "                patch_size: int = 16,\n",
        "                emb_size: int = 768,\n",
        "                img_size: int = 224,\n",
        "                depth: int = 12,\n",
        "                n_classes: int = 2,\n",
        "                **kwargs):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
        "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
        "            ClassificationHead(emb_size, n_classes)\n",
        "        )"
      ],
      "metadata": {
        "id": "gsKryS7hTrZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, data):\n",
        "    correct, total = 0, 0\n",
        "    for sms, labels in data:\n",
        "        output = model(sms[0])\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += labels.shape[0]\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "LX688Qjqsgge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_network(model, train, valid, num_epochs=5, batch_size = 64, learning_rate=1e-5):\n",
        "    use_cuda = True\n",
        "\n",
        "    print(\"epoch 0\")\n",
        "    optimizer = optim.Adam(model.parameters(), learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "\n",
        "    losses, train_acc, valid_acc = [], [], []\n",
        "    epochs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"ok\")\n",
        "        # for image in iter(train_loader):\n",
        "        #     optimizer.zero_grad()\n",
        "        #     # model.float()\n",
        "        #     pred = model(image)\n",
        "        #     loss = criterion(pred, image)\n",
        "        #     loss.backward()\n",
        "        #     optimizer.step()\n",
        "        for imgs, labels in iter(train):\n",
        "\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              imgs = imgs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            \n",
        "            # imgs = torch.from_numpy(imgs)\n",
        "            # imgs = torch.unsqueeze(imgs,0)\n",
        "            # imgs = imgs.numpy()\n",
        "            print(imgs.shape)\n",
        "            #labels = np.reshape(labels, (1,1))\n",
        "            # labels = torch.tensor(labels, dtype=torch.int8)\n",
        "            out = model(imgs)             # forward pass\n",
        "            # out = torch.flatten(out)\n",
        "            # out = out.type(torch.LongTensor)\n",
        "            print(out.shape, labels.shape)\n",
        "\n",
        "            loss = criterion(out, labels) # compute the total loss\n",
        "            loss.backward()               # backward pass (compute parameter updates)\n",
        "            optimizer.step()              # make the updates for each parameter\n",
        "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "        \n",
        "        losses.append(loss)\n",
        "        epochs.append(epoch+1)\n",
        "        train_acc.append(get_accuracy(model, train))\n",
        "        valid_acc.append(get_accuracy(model, valid))\n",
        "        print(\"Epoch %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
        "              epoch+1, loss, train_acc[-1], valid_acc[-1]))\n",
        "\n",
        "\n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(losses, label=\"Train\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(epochs, train_acc, label=\"Train\")\n",
        "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UWcqby1usisv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = VisionTransformer()\n",
        "train_network(net, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfKXK8hj4Xir",
        "outputId": "319495c4-76c2-4eaa-95d9-544038af1904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0\n",
            "ok\n",
            "torch.Size([100, 3, 224, 224])\n"
          ]
        }
      ]
    }
  ]
}