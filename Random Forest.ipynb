{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JEN6YT/APS360-Project/blob/main/Random%20Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEZbhYA4rCGi"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import pandas as pd\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu0sPIvgrSbL",
        "outputId": "f6abf220-334c-4d07-a278-6ef9dedc9395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get image and label\n"
      ],
      "metadata": {
        "id": "S7KIw4ei_g8P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvsKOIfpxWu3"
      },
      "outputs": [],
      "source": [
        "# read the xlsx file from Google Drive\n",
        "file_path = '/content/drive/My Drive/U of T/APS360 Deep Learning/NIH-NLM-ThinBloodSmearsPf/img_path.xlsx'\n",
        "\n",
        "# The dataframe store all the path, i.e. 'NIH-NLM-ThinBloodSmearsPf\\Polygon Set\\142C38P...'\n",
        "df1 = pd.read_excel(file_path)\n",
        "\n",
        "# create complete path for each image\n",
        "prefix = \"/content/drive/My Drive/U of T/APS360 Deep Learning/\"\n",
        "df1 = df1.applymap(lambda x: prefix + str(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4HfcIixyB4p"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVJRao1kgYq-",
        "outputId": "b9245b08-1370-4b8a-8263-8fb8037134dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the xlsx file from Google Drive\n",
        "file_path_infected = '/content/drive/My Drive/U of T/APS360 Deep Learning/NIH-NLM-ThinBloodSmearsPf/infected_RBC.xlsx'\n",
        "\n",
        "# The dataframe store all the path, i.e. 'NIH-NLM-ThinBloodSmearsPf\\Polygon Set\\142C38P...'\n",
        "df2 = pd.read_excel(file_path_infected)"
      ],
      "metadata": {
        "id": "7YtfiqUfVoPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "Jrg_UbI_V6gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.merge(df1, df2, left_index=True, right_index=True)"
      ],
      "metadata": {
        "id": "ImDUqmLEV-1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "WvQ9infyWX-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "OZ80jCP9gito"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label, resize, save in array"
      ],
      "metadata": {
        "id": "Z3ZEl0XjQbyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = []\n",
        "target_size = (224, 224)\n",
        "\n",
        "# iterate over rows and label images\n",
        "for i, row in df.iterrows():\n",
        "  # get the file path and infected RBC count\n",
        "  print(i)\n",
        "\n",
        "  file_path = row['File path']\n",
        "  infected_rbc = row['Infected RBC']\n",
        "  \n",
        "  # open the image using PIL\n",
        "  image = Image.open(file_path)\n",
        "  image = image.resize(target_size)\n",
        "  img_arr = np.array(image)\n",
        "  \n",
        "  # label the image based on the infected RBC count\n",
        "  if infected_rbc == 0:\n",
        "    label = 0\n",
        "  elif infected_rbc > 0:\n",
        "    label = 1\n",
        "  \n",
        "  # append the image and label to the list\n",
        "  data = {'image': img_arr, 'label': label}\n",
        "  data_list.append(data)\n",
        "\n",
        "# convert the list to numpy array and save it\n",
        "np.save('data.npy', np.array(data_list))"
      ],
      "metadata": {
        "id": "VMJLlrZ5gOvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list[0]"
      ],
      "metadata": {
        "id": "4NwE52_7xqJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking balance"
      ],
      "metadata": {
        "id": "EJZ-96Mm8bT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "infected = 0\n",
        "uninfected = 0\n",
        "for data in data_list:\n",
        "  if data['label'] == 1:\n",
        "    infected += 1\n",
        "  if data['label'] == 0:\n",
        "    uninfected += 1\n",
        "total = infected+uninfected\n",
        "print(f'infected data {infected/total} and uninfected data {uninfected/total}')"
      ],
      "metadata": {
        "id": "klEeUvM2zpRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resampling"
      ],
      "metadata": {
        "id": "Ht2S7B0YQNGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data from the saved numpy array\n",
        "data = np.load('data.npy', allow_pickle=True)\n",
        "\n",
        "# get the features (images) and labels\n",
        "image = np.array([d['image'].flatten() for d in data])\n",
        "label = np.array([d['label'] for d in data])\n",
        "\n",
        "# reshape X to a 2D array\n",
        "image = image.reshape(image.shape[0], -1)\n",
        "\n",
        "# apply RandomOverSampler to X and y\n",
        "ros = RandomOverSampler()\n",
        "image_resampled, label_resampled = ros.fit_resample(image, label)\n",
        "\n",
        "# reshape X_resampled back to 4D array\n",
        "image_resampled = image_resampled.reshape(image_resampled.shape[0], 224, 224, 3)\n",
        "\n",
        "# combine X_resampled and y_resampled into a list of dicts\n",
        "data_resampled = [{'image': image_resampled[i], 'label': label_resampled[i]} for i in range(len(label_resampled))]\n",
        "\n",
        "# save the resampled data to a new numpy array\n",
        "np.save('data_resampled.npy', np.array(data_resampled))"
      ],
      "metadata": {
        "id": "577qfIbMMyv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infected = 0\n",
        "uninfected = 0\n",
        "for data in data_resampled:\n",
        "  if data['label'] == 1:\n",
        "    infected += 1\n",
        "  if data['label'] == 0:\n",
        "    uninfected += 1\n",
        "total = infected+uninfected\n",
        "print(f'infected data {infected/total} and uninfected data {uninfected/total}')"
      ],
      "metadata": {
        "id": "mBQbE3Orm72E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total"
      ],
      "metadata": {
        "id": "vew0lW0Om-fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalizing"
      ],
      "metadata": {
        "id": "oImGtapqeBam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the normalization function\n",
        "def normalize(image):\n",
        "  return (image - np.min(image)) / (np.max(image) - np.min(image))\n",
        "\n",
        "# Normalize the data\n",
        "normalized_data = []\n",
        "for data in data_resampled:\n",
        "  normalized_image = normalize(data['image'])\n",
        "  normalized_data.append({'image': normalized_image, 'label': data['label']})\n",
        "\n",
        "# Save the normalized data as a .npy file\n",
        "np.save('normalized_data.npy', normalized_data)\n",
        "#print(normalized_data)"
      ],
      "metadata": {
        "id": "C5aVsu02fQcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting"
      ],
      "metadata": {
        "id": "RaRBEnGCQn2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the features (images) and labels\n",
        "X = np.array([d['image'] for d in normalized_data])\n",
        "y = np.array([d['label'] for d in normalized_data])\n",
        "\n",
        "# Split your data into training, validation and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=66)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=66)"
      ],
      "metadata": {
        "id": "EDCqgFiS4IiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train),len(X_val),len(X_test))"
      ],
      "metadata": {
        "id": "HFe6gJgfhwJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "GXfW1tS4C1Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "lmduqWQogy7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an ImageDataGenerator instance with desired data augmentation parameters\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=25,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# compute internal statistics of the data\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# generate augmented data in batches\n",
        "augmented_data_generator = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "# iterate over batches of augmented data and add them to the original dataset\n",
        "for x_batch, y_batch in augmented_data_generator:\n",
        "  X_train = np.concatenate([X_train, x_batch], axis=0)\n",
        "  y_train = np.concatenate([y_train, y_batch], axis=0)\n",
        "  \n",
        "  # break the loop if we have reached the desired number of samples\n",
        "  if len(X_train) >= 1348:\n",
        "      break"
      ],
      "metadata": {
        "id": "_d9qz2b9CKv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the random seed for reproducibility\n",
        "np.random.seed(66)\n",
        "\n",
        "# create an array of indices for shuffling\n",
        "indices = np.arange(len(X_train))\n",
        "\n",
        "# shuffle the indices\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# shuffle the x_train and y_train arrays using the shuffled indices\n",
        "X_train = X_train[indices]\n",
        "y_train = y_train[indices]"
      ],
      "metadata": {
        "id": "dgF0XkS-EHJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOFioDTXg3HX",
        "outputId": "438f96fa-4d32-479e-9bff-997d49b25ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1370, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H9QTbhfiRee",
        "outputId": "7d634bfa-11ab-417c-f9d9-e818dc6ff019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1370,)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten the input data\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "ytrain = y_train.reshape(-1,1)\n",
        "ytest = y_test.reshape(-1,1)"
      ],
      "metadata": {
        "id": "fZrSsrL_jW_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = np.load('/content/drive/My Drive/U of T/APS360 Deep Learning/x_train_data.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "vbY0s_BljG9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train = np.load('/content/drive/My Drive/U of T/APS360 Deep Learning/y_train_data.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "t1jwcMS-jHyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_val = np.load('/content/drive/My Drive/U of T/APS360 Deep Learning/x_val_data.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "uGGIamcnjKsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_val = np.load('/content/drive/My Drive/U of T/APS360 Deep Learning/y_val_data.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "2xOHn4uLjMP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test = np.load('/content/drive/My Drive/U of T/APS360 Deep Learning/x_test_data.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "SXsnmUBZEod3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_test = np.load('/content/drive/My Drive/U of T/APS360 Deep Learning/y_test_data.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "Wpq8bhFtEs1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model"
      ],
      "metadata": {
        "id": "Ft-kG3lQVVfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline Model: Random Forest\n",
        "\n",
        "Create a RandomForestClassifier with 100 decision trees and train the model using the training data with rf.fit(X_train, y_train).\n",
        "\n",
        "After training the model, we use it to predict the target variable for the test data with y_pred = rf.predict(X_test).\n",
        "\n",
        "Finally, we evaluate the accuracy of the model using accuracy_score and print the result."
      ],
      "metadata": {
        "id": "ovdTuLZ9ergW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a Random Forest classifier with 10 decision trees\n",
        "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the Random Forest classifier\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target variable for the test data\n",
        "y_pred = rf.predict(X_test)"
      ],
      "metadata": {
        "id": "WI2JrIReVf0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "AzG_a2SSro9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "\n",
        "# Plot metrics\n",
        "metrics = {'MSE': mse, 'MAE': mae}\n",
        "plt.bar(metrics.keys(), metrics.values())\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ByaKOkM0qIQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example from: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# flatten the input data\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "ytrain = y_train.reshape(-1,1)\n",
        "ytest = y_test.reshape(-1,1)\n",
        "\n",
        "class_names = [0, 1]\n",
        "\n",
        "# Run classifier, using a model that is too regularized (C too low) to see\n",
        "# the impact on the results\n",
        "classifier = svm.SVC(kernel=\"linear\", C=0.01).fit(X_train, ytrain)\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "titles_options = [\n",
        "    (\"Confusion matrix, without normalization\", None),\n",
        "    (\"Normalized confusion matrix\", \"true\"),\n",
        "]\n",
        "\n",
        "# after normalizing, it becomes probability (ex. of the versicolor row, 0.62 of the row is predicted and classified as versicolor)\n",
        "for title, normalize in titles_options:\n",
        "    disp = ConfusionMatrixDisplay.from_estimator(\n",
        "        classifier,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        display_labels=class_names,\n",
        "        cmap=plt.cm.Blues,\n",
        "        normalize=normalize,\n",
        "    )\n",
        "    disp.ax_.set_title(title)\n",
        "\n",
        "    print(title)\n",
        "    print(disp.confusion_matrix)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hhy6-kBSCwJf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}